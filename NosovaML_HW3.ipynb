{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NosovaML_HW3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMLu7EJa4KErykXpLgeoz73",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElizavetaNosova/HSE_ML_homework/blob/master/NosovaML_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofazrft4JP0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "categories = ['sci.crypt', 'sci.med', 'comp.graphics', 'talk.politics.guns']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
        "vectors = vectorizer.fit_transform(newsgroups_train.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c6oHH1REI_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpSTb1oaDRTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from statistics import mean\n",
        "X = vectors\n",
        "y = newsgroups_train.target\n",
        "X, X_test, y, y_test = train_test_split(X, y, random_state=42)\n",
        "folds = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "def cross_validation(model):\n",
        "    valid_scores = []\n",
        "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
        "        X_train, X_valid = X[train_index], X[valid_index]\n",
        "        y_train, y_valid = y[train_index], y[valid_index]\n",
        "        model.fit(X_train, y_train)    \n",
        "        y_pred_test = model.predict(X_test)\n",
        "        y_pred_valid = model.predict(X_valid)\n",
        "        valid_score = sklearn.metrics.f1_score(y_valid, y_pred_valid, average='macro')\n",
        "        valid_scores.append(valid_score)\n",
        "    test_score = sklearn.metrics.f1_score(y_test, y_pred_test, average='macro')\n",
        "    print('Cross Validation mean score:', mean(valid_scores))\n",
        "    print(\"Score on test data: {0:.4f}\".format(test_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BUfW9DoYG0f",
        "colab_type": "code",
        "outputId": "ceda043a-acd0-4eed-d3a6-766d32acd9d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 2, 0, ..., 1, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg4J41uwp3fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters_LogisticRegression = {'class_weight' : ['balanced', None],\n",
        "                  'C' : [0.01, 0.1, 1.0, 10.0],\n",
        "                  'max_iter': [500, 1000]\n",
        "                 }\n",
        "\n",
        "parameters_RandomForest = {'n_estimators': [5, 10, 15],\n",
        "                           'max_depth': [None, 5],\n",
        "                           'min_samples_split': [0.5, 1.0, 5]                          \n",
        "                }\n",
        "\n",
        "parameters_kNeigbors = {'n_neighbors': [3, 5, 10],\n",
        "                        'p': [1,2]\n",
        "                }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETaPBSNIK_qF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "def grid_search(model, parameters):\n",
        "    grid_search = GridSearchCV(model, param_grid=parameters, cv=folds, scoring='f1_macro')\n",
        "    grid_search.fit(X, y)\n",
        "    print('Best score: {}'.format(grid_search.best_score_))\n",
        "    print('Best parameters: {}'.format(grid_search.best_params_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqH2tcVyVGQS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "830022dc-bf35-4b4e-951d-ea50cd737877"
      },
      "source": [
        "from sklearn import linear_model\n",
        "grid_search(linear_model.LogisticRegression(), parameters_LogisticRegression)\n",
        "#Я случайно перезапустила ячейку, найденные параметры выписаны ниже"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-014e4420e0af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_LogisticRegression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-101dd10d0a6b>\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(model, parameters)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1_macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best score: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best parameters: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1599\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1601\u001b[0;31m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L-BFGS-B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m                 \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"iprint\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gtol\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"maxiter\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m             )\n\u001b[1;32m    938\u001b[0m             n_iter_i = _check_optimize_result(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    336\u001b[0m         _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr,\n\u001b[1;32m    337\u001b[0m                        \u001b[0mpgtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miwa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsave\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                        isave, dsave, maxls)\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mtask_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'FG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFY2wKJlc07d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a3a89def-2e96-497d-985e-77d5925c8d06"
      },
      "source": [
        "from sklearn import ensemble\n",
        "grid_search(ensemble.RandomForestClassifier(), parameters_RandomForest)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.8604137830119477\n",
            "Best parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 15}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPZp4RHBdn18",
        "colab_type": "code",
        "outputId": "0c3517b3-f916-4834-c8e6-8b203bece1c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "grid_search(sklearn.neighbors.KNeighborsClassifier(), parameters_kNeigbors)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.6943276662309239\n",
            "Best parameters: {'n_neighbors': 3, 'p': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN-QHedveqym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chosenLogisticRegression = linear_model.LogisticRegression(C=1.0, class_weight=None, max_iter=500)\n",
        "chosenRandomForest = ensemble.RandomForestClassifier(max_depth=None, min_samples_split=5, n_estimators=15)\n",
        "chosenKNeigbors = sklearn.neighbors.KNeighborsClassifier(n_neighbors = 3, p = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_yhlJJRfgor",
        "colab_type": "code",
        "outputId": "de8d82ec-d8f6-4d47-f86f-cb9ddaebdf32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cross_validation(chosenLogisticRegression)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtgrJj5yk3b0",
        "colab_type": "code",
        "outputId": "c2e53c0f-0dd1-42a7-def0-fd0d94550085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cross_validation(chosenRandomForest)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Validation mean score: 0.8651362356906477\n",
            "Score on test data: 0.8595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFoGg30knEbJ",
        "colab_type": "code",
        "outputId": "9ce7fbb8-8d26-4400-fece-946a6daf694f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cross_validation(chosenKNeigbors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-uFjQOQnPr0",
        "colab_type": "text"
      },
      "source": [
        "Результаты предсказаний на тесте могут отличаться от результатов кросс-валдации как в большую, так и в меньшую сторону, но не существенно (в пределах 1,5 процентов). На какое бы число мы ни ориентировались, лучшие результаты даёт логистическая регрессия, худшие - K ближайших соседей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTHDqcOkobgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import eli5  \n",
        "import pandas as pd  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0etCHqct33E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LRmodel = chosenLogisticRegression.fit(X, y)\n",
        "RFmodel = chosenRandomForest.fit(X,y)\n",
        "KNmodel = chosenKNeigbors.fit(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVpslcaZuYPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p4tBGbMIAv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = eli5.formatters.as_dataframe.explain_weights_df(LRmodel)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AjWTCZ7upuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analyze_features(model, n=10):\n",
        "    df = eli5.formatters.as_dataframe.explain_weights_df(model)\n",
        "    for i in range(len(categories)):\n",
        "        category_df = df[df['target']==i]\n",
        "        print('КАТЕГОРИЯ', i)\n",
        "        category_df = category_df.sort_values(['weight'], ascending=False)\n",
        "        ids = list(category_df['feature'])\n",
        "        printed = 0\n",
        "        for i in ids:\n",
        "            if printed == n:\n",
        "                break\n",
        "            else:\n",
        "                try:  #как минимум индекс BIAS возможен в списке, но его нет в словаре\n",
        "                    key = int(i[1:])\n",
        "                    print(index_to_word[key])\n",
        "                    printed += 1\n",
        "                except:\n",
        "                    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8ILSRKMxHPR",
        "colab_type": "code",
        "outputId": "a254e875-2e2c-45af-bb14-8b8b5642bdf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "analyze_features(LRmodel, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "КАТЕГОРИЯ 0\n",
            "graphics\n",
            "image\n",
            "3d\n",
            "files\n",
            "images\n",
            "looking\n",
            "software\n",
            "points\n",
            "file\n",
            "card\n",
            "КАТЕГОРИЯ 1\n",
            "clipper\n",
            "key\n",
            "encryption\n",
            "chip\n",
            "gtoal\n",
            "security\n",
            "pgp\n",
            "nsa\n",
            "code\n",
            "keys\n",
            "КАТЕГОРИЯ 2\n",
            "doctor\n",
            "msg\n",
            "pitt\n",
            "my\n",
            "disease\n",
            "health\n",
            "photography\n",
            "treatment\n",
            "dyer\n",
            "information\n",
            "КАТЕГОРИЯ 3\n",
            "gun\n",
            "guns\n",
            "waco\n",
            "batf\n",
            "atf\n",
            "usa\n",
            "firearms\n",
            "cathy\n",
            "rights\n",
            "them\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zP7wcENIzR3",
        "colab_type": "text"
      },
      "source": [
        "В модели логистической регрессии подборки слов позволяют однозначно узнать изначальную тему).\n",
        "Видимо, номера тем не совпадают с порядком, в котором я их указывала: медицина была второй (первой, считая от нуля), а стала предпоследней темой.\n",
        "Есть слова, отнесённые не в ту группу (photography попало в медицину). С другими категориями никаких проблем нет. Слово them попало в категорию оружия, вероятно, в связи с тем, что в текстах этой категории чаще всего совершают действия, направленные на несколько человек (в остальных случаях объект чаще неодушевлённый или - в случае с медициной - 1 человек)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_Fw0YTnPvVn",
        "colab_type": "code",
        "outputId": "6905ab86-ce48-4319-f9b9-d52e5c7afe61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "##Повторим обучение, оставив только 500 самых важных слов\n",
        "df = eli5.formatters.as_dataframe.explain_weights_df(LRmodel)\n",
        "df = df.sort_values(['weight'], ascending=False)\n",
        "important_featuresLR = list(df.feature)[:500]\n",
        "important_wordsLR = []\n",
        "for i in important_featuresLR:\n",
        "    try:\n",
        "        important_wordsLR.append(index_to_word[int(i[1:])])\n",
        "    except:\n",
        "        pass\n",
        "print(important_wordsLR)\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['gun', 'clipper', 'graphics', 'key', 'guns', 'encryption', 'chip', 'waco', 'gtoal', 'security', 'doctor', 'pgp', 'msg', 'image', 'pitt', 'batf', 'my', '3d', 'atf', 'files', 'usa', 'disease', 'firearms', 'images', 'looking', 'nsa', 'code', 'software', 'points', 'keys', 'file', 'card', 'crypto', 'des', 'health', 'package', 'photography', 'treatment', 'once', 'cathy', 'dyer', 'information', 'cview', 'tiff', '3do', 'windows', 'gif', 'mail', 'cryptography', 'na', 'hi', 'banks', 'people', 'secret', 'tapped', 'polygon', 'netcom', 'your', 'radford', 'rights', '42', 'she', 'advance', '24', 'them', 'geb', 'effects', 'feustel', 'program', 'hp', 'smith', 'rice', 'computer', 'us', 'texas', 'format', 'krillean', 'access', 'should', 'eff', 'tx', 'sgi', 'fbi', 'public', 're', 'marc', 'vesa', 'color', 'post', 'vga', 'jeeves', 'missouri', 'pollux', 'book', 'politics', 'need', 'conference', 'gordon', 'video', 'tempest', 'ranch', 'medical', 'eye', 'nagle', 'animation', 'more', 'wiretap', 'au', 'research', 'distribution', 'out', 'escrow', 'virtual', 'toal', 'medicine', 'well', 'ac', 'ucsd', 'foods', 'surface', 'writes', 'who', 'blood', 'cdt', 'robert', 'sometimes', 'qualcomm', 'suite', 'might', 'library', 'handgun', 'erythromycin', 'amendment', 'pain', 'imagine', 'packard', 'hewlett', 'weapons', 'sphere', 'liberty', 'antibiotics', 'food', 'almanac', 'killed', 'rsa', 'roxonal', 'stratus', 'michael', 'med', 'yeast', 'tis', 'thanks', 'fractal', 'ibm', 'columbia', 'berkeley', 'clinton', 'scientific', 'usc', 'cray', 'cancer', 'bruce', 'ftp', 'routine', 'jack', 'other', 'pov', 'boi', '2nd', 'short', 'huey', 'schneier', 'issa', 'line', 'org', 'keyboard', 'word', 'usually', 'thank', 'fractals', 'source', 'remember', 'driver', 'needles', 'dividian', 'burns', 'foreskin', 'david', 'jmetz', 'weapon', 'freedom', 'phone', 'claussen', 'code', '19', 'george', 'recently', 'gas', 'rumours', 'jed', 'similar', 'idea', 'useful', 'system', 'umich', 'carson', 'graham', 'yale', 'where', 'speedstar', 'liver', 'science', 'government', 'candida', 'secure', 'john', 'part', 'miller', 'survivors', 'aids', 'shots', 'sure', 'll', 'using', 'move', 'msu', 'picture', 'skin', 'carry', 'privacy', 'cadence', 'gunsmithing', 'locus', 'government', 'me', 'cl', 'products', 'imaging', 'hci', 'second', 'paul', 'machine', 'such', 'cost', 'or', 'caltech', 'portal', 'mit', 'prozac', 'dave', 'point', 'classified', 'another', 'address', 'reasoning', 'vectors', 'vak12ed', 'also', 'walker', 'national', 'chinet', 'cup', 'seizures', 'diet', 'crime', 'nra', 'ls8139', 'colorado', 'reality', 'ifas', 'gnv', 'zisfein', 'communications', 'encrypted', 'run', 'shograf', 'tron', 'sternlight', 'systems', 'gatech', 'jr0930', 'lines', 'world', 'cwru', 'email', 'quality', 'roby', 'is', 'password', 'syndrome', 'help', 'must', 'treating', 'no', 'compression', 'acsc', 'denning', 'so', 'only', 'law', 'bontchev', 'there', 'school', 'siemens', 'take', 'counselor', 'support', 'press', 'auburn', 'york', 'amiga', 'happen', 'de', 'split', 'oldham', 'but', 'idaho', 'criminals', 'gritz', 'printer', 'wiretaps', 'did', 'place', 'friend', 'risk', 'neil', 'xv', 'rpi', 'carl', 'private', 'senator', 'algorithm', 'noring', 'anyone', 'earth', 'physician', '68070', 'cmuvm', 'note', 'come', 'time', 'does', 'nl', 'raid', 'police', 'machines', 'boise', 'georgia', 'cnsvax', 'tea4two', 'philosophical', 'calstate', 'eis', 'chopin', 'ufl', '12', 'sound', 'announcement', 'after', 'display', 'glover', 'lethal', 'udel', 'pc', 'uk', 'gsfc', 'firearm', 'cell', 'ai', 'betz', 'high', 'spots', 'patient', 'euclid', 'cold', 'now', 'version', 'ucs', 'regulated', 'col', 'jupiter', 'taste', 'uwec', 'bill', 'purdue', 'questions', 'he', 'scws8', 'someone', 'war', 'foulston', 'cathyf', 'law', 'cd', 'ebcdic', 'spdcc', 'safety', 'wrote', 'ca', 'right', 'rocket', 'text', 'fever', 'mizzou1', 'dsto', 'hp', 'long', 'siggraph', 'crohn', 'utexas', 'which', 's2', 'eng', 'yes', 'kathleen', 'ucrengr', 'screen', 'grayscale', 'responsibility', 'daypro', 'oxaprozin', 'karicha', 'decision', 'ins', 'sleep', 'their', 'significance', 'data', 'old', 'roth', 'mode', 'naomi', 'chemical', 'problem', 'engin', 'certainly', 'kde', 'emmen', 'wrong', 'gozer', 'richards', 'calls', 'ati', 'handheld', 'cure', 'polygons', 'side', 'keith', 'curtin', 'lee', 'chuck', 'matt', 's1', 'talk', 'hilarie', 'orman', 'were', 'could', 'roos', 'hard', 'phillips', 'dear', 'rita', 'canada', 'seems', 'passing', 'falcon', 'eldar', 'over', 'these', 'do', 'next', 'melissa', 'california', 'montemayor', 'abrash', 'gmontem', 'kadie', 'cnn', 'control', 'cleveland', 'messages', 'distribution', 'recall', 'wpi', 'pasadena', 'gov', 'arythmia', 'helsinki', 'sauce', 'his', 'how', 'fiberman', 'wsun', 'trinomials', 'somebody', 'diablo', 'education']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms7RdW9CUGYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def pseudotext(words, text):\n",
        "    tokens = [token for token in re.findall('[a-zA-Z]+', text) if token in words]\n",
        "    pseudotext = ' '.join(tokens)\n",
        "    return pseudotext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9osAui-WsPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "important_words_data = [pseudotext(important_wordsLR, text) for text in newsgroups_train.data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpV_pTcIQ0Jx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = vectorizer.transform(important_words_data)\n",
        "y = newsgroups_train.target\n",
        "X, X_test, y, y_test = train_test_split(X, y, random_state=42)\n",
        "folds = KFold(n_splits=5, shuffle=True, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYJx9qBIY8F6",
        "colab_type": "code",
        "outputId": "00dc624a-f8bb-407c-bc15-f4824129d013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "grid_search(linear_model.LogisticRegression(), parameters_LogisticRegression)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.933409456986543\n",
            "Best parameters: {'C': 1.0, 'class_weight': 'balanced', 'max_iter': 500}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW201IlYaL6P",
        "colab_type": "code",
        "outputId": "93c77758-8db6-4e84-b167-6687fcb79b73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cross_validation(linear_model.LogisticRegression(C=1.0, class_weight='balanced', max_iter=500))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Validation mean score: 0.933409456986543\n",
            "Score on test data: 0.9032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uytaMh_EavCl",
        "colab_type": "text"
      },
      "source": [
        "Переобучение стало проявляться сильнее (больше разница между валидацией и тестом), качество работы ухудшилось. \n",
        "Проверим, не связано ли это с тем, что стало много пустых псевдотекстов, которые можно распределить по категориям только наугад"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZhXjWP0cFT4",
        "colab_type": "code",
        "outputId": "1cebe2f8-3535-40cf-c3b6-25c869d849dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len([text for text in important_words_data if not text])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bybrZMCPcekx",
        "colab_type": "text"
      },
      "source": [
        "Не связано."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qaCKABUcjoz",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sLXN4nTYy5d",
        "colab_type": "text"
      },
      "source": [
        "Случайный лес. Метод feature_importances_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8cN2lX6KPGt",
        "colab_type": "code",
        "outputId": "5dbca651-e803-404a-c9c9-e1f40573c413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "RFmodel.feature_importances_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00012498, 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "       0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgeyd5k3ArM-",
        "colab_type": "text"
      },
      "source": [
        "С помощью функции feature_importances_, не получается понять, к какому решению склоняет какое слово.\n",
        "Посмотрим на слова с ненулевым значением\n",
        "Если они будут похожи на соответствующие выбранным темам, натренируем модель с теми же параметрами на выявление каждой темы по отдельности, чтобы посмотреть списки слов по тематическим категориям"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6GmFRdcNipI",
        "colab_type": "code",
        "outputId": "3822a405-b9f8-43a7-e48f-d13ab332267d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "importances = RFmodel.feature_importances_\n",
        "not_zero = [i for i in importances if i]\n",
        "print(len(not_zero))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdv2TEHldIki",
        "colab_type": "text"
      },
      "source": [
        "Ненулевых элементов достаточно много.\n",
        "Посмотрим, сколько элементов вносит не менее 1.5 от среднего значения и какие это слова"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTw9fWNZdnmp",
        "colab_type": "code",
        "outputId": "bda33333-6aca-4f9b-e07a-2684713ae3d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import statistics\n",
        "mean = statistics.mean(not_zero)\n",
        "print(len([i for i in not_zero if i > 1.5*mean]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "440\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHIeLUjxeVhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "important_wordsRT = []\n",
        "for i in range(len(importances)):\n",
        "    if importances[i] > 1.5*mean:\n",
        "        important_wordsRT.append(index_to_word[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UliNtVqsfUum",
        "colab_type": "code",
        "outputId": "2794b98b-78a2-420b-e611-ba6421518adf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "important_wordsRT"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['15',\n",
              " '24',\n",
              " '256',\n",
              " '3d',\n",
              " '54',\n",
              " '7dv',\n",
              " '80',\n",
              " 'able',\n",
              " 'about',\n",
              " 'access',\n",
              " 'agencies',\n",
              " 'agents',\n",
              " 'agree',\n",
              " 'algorithm',\n",
              " 'all',\n",
              " 'allergic',\n",
              " 'am',\n",
              " 'amendment',\n",
              " 'an',\n",
              " 'and',\n",
              " 'anecdotal',\n",
              " 'animation',\n",
              " 'announcement',\n",
              " 'any',\n",
              " 'appears',\n",
              " 'appreciated',\n",
              " 'are',\n",
              " 'argue',\n",
              " 'as',\n",
              " 'atf',\n",
              " 'attacks',\n",
              " 'auto',\n",
              " 'available',\n",
              " 'banks',\n",
              " 'batf',\n",
              " 'be',\n",
              " 'been',\n",
              " 'believe',\n",
              " 'bellovin',\n",
              " 'between',\n",
              " 'bill',\n",
              " 'body',\n",
              " 'boi',\n",
              " 'bontchev',\n",
              " 'book',\n",
              " 'both',\n",
              " 'branch',\n",
              " 'building',\n",
              " 'burns',\n",
              " 'but',\n",
              " 'c5rpoj',\n",
              " 'ca',\n",
              " 'caltech',\n",
              " 'can',\n",
              " 'candida',\n",
              " 'car',\n",
              " 'cards',\n",
              " 'case',\n",
              " 'cc',\n",
              " 'cdt',\n",
              " 'cell',\n",
              " 'chaos',\n",
              " 'cheap',\n",
              " 'child',\n",
              " 'chip',\n",
              " 'choose',\n",
              " 'chopin',\n",
              " 'chronic',\n",
              " 'citizens',\n",
              " 'claim',\n",
              " 'client',\n",
              " 'clinical',\n",
              " 'clinton',\n",
              " 'clipper',\n",
              " 'code',\n",
              " 'cold',\n",
              " 'color',\n",
              " 'colorado',\n",
              " 'com',\n",
              " 'communications',\n",
              " 'computer',\n",
              " 'concealed',\n",
              " 'cont',\n",
              " 'control',\n",
              " 'conversations',\n",
              " 'court',\n",
              " 'crime',\n",
              " 'criminal',\n",
              " 'criminals',\n",
              " 'crypt',\n",
              " 'crypto',\n",
              " 'cs',\n",
              " 'cwru',\n",
              " 'data',\n",
              " 'daughter',\n",
              " 'day',\n",
              " 'days',\n",
              " 'defense',\n",
              " 'des',\n",
              " 'device',\n",
              " 'did',\n",
              " 'didn',\n",
              " 'diet',\n",
              " 'disease',\n",
              " 'diseases',\n",
              " 'display',\n",
              " 'distribution',\n",
              " 'do',\n",
              " 'doctor',\n",
              " 'doctors',\n",
              " 'don',\n",
              " 'door',\n",
              " 'dorothy',\n",
              " 'due',\n",
              " 'during',\n",
              " 'eating',\n",
              " 'edu',\n",
              " 'eff',\n",
              " 'effect',\n",
              " 'effective',\n",
              " 'effects',\n",
              " 'email',\n",
              " 'encrypted',\n",
              " 'encryption',\n",
              " 'escrow',\n",
              " 'especially',\n",
              " 'ever',\n",
              " 'everyone',\n",
              " 'fact',\n",
              " 'far',\n",
              " 'fbi',\n",
              " 'fcrary',\n",
              " 'few',\n",
              " 'file',\n",
              " 'files',\n",
              " 'find',\n",
              " 'fire',\n",
              " 'firearm',\n",
              " 'firearms',\n",
              " 'food',\n",
              " 'foods',\n",
              " 'for',\n",
              " 'force',\n",
              " 'free',\n",
              " 'ftp',\n",
              " 'generally',\n",
              " 'georgia',\n",
              " 'getting',\n",
              " 'gif',\n",
              " 'gone',\n",
              " 'good',\n",
              " 'gordon',\n",
              " 'government',\n",
              " 'graham',\n",
              " 'graphics',\n",
              " 'guess',\n",
              " 'gun',\n",
              " 'guns',\n",
              " 'guy',\n",
              " 'hackers',\n",
              " 'had',\n",
              " 'hamburg',\n",
              " 'handgun',\n",
              " 'handheld',\n",
              " 'handling',\n",
              " 'happen',\n",
              " 'hardware',\n",
              " 'harvard',\n",
              " 'has',\n",
              " 'have',\n",
              " 'haven',\n",
              " 'he',\n",
              " 'head',\n",
              " 'healthy',\n",
              " 'help',\n",
              " 'here',\n",
              " 'hi',\n",
              " 'high',\n",
              " 'his',\n",
              " 'holland',\n",
              " 'home',\n",
              " 'house',\n",
              " 'how',\n",
              " 'hp',\n",
              " 'if',\n",
              " 'illinois',\n",
              " 'illness',\n",
              " 'image',\n",
              " 'images',\n",
              " 'in',\n",
              " 'information',\n",
              " 'intellect',\n",
              " 'into',\n",
              " 'investors',\n",
              " 'is',\n",
              " 'isn',\n",
              " 'it',\n",
              " 'its',\n",
              " 'james',\n",
              " 'jim',\n",
              " 'jmd',\n",
              " 'keep',\n",
              " 'key',\n",
              " 'keys',\n",
              " 'keywords',\n",
              " 'killed',\n",
              " 'kkk',\n",
              " 'know',\n",
              " 'koresh',\n",
              " 'krillean',\n",
              " 'law',\n",
              " 'lawrence',\n",
              " 'let',\n",
              " 'liberty',\n",
              " 'like',\n",
              " 'likely',\n",
              " 'linknet',\n",
              " 'local',\n",
              " 'long',\n",
              " 'looking',\n",
              " 'machine',\n",
              " 'mail',\n",
              " 'make',\n",
              " 'management',\n",
              " 'mass',\n",
              " 'may',\n",
              " 'me',\n",
              " 'media',\n",
              " 'medicine',\n",
              " 'messages',\n",
              " 'might',\n",
              " 'mike',\n",
              " 'much',\n",
              " 'my',\n",
              " 'mykotronx',\n",
              " 'n3jxp',\n",
              " 'n9myi',\n",
              " 'na',\n",
              " 'natural',\n",
              " 'need',\n",
              " 'needles',\n",
              " 'network',\n",
              " 'never',\n",
              " 'new',\n",
              " 'nj',\n",
              " 'no',\n",
              " 'nobody',\n",
              " 'not',\n",
              " 'nova',\n",
              " 'nra',\n",
              " 'of',\n",
              " 'often',\n",
              " 'on',\n",
              " 'once',\n",
              " 'one',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'out',\n",
              " 'ovens',\n",
              " 'over',\n",
              " 'owners',\n",
              " 'pa146008',\n",
              " 'packet',\n",
              " 'pain',\n",
              " 'pasadena',\n",
              " 'passwords',\n",
              " 'patent',\n",
              " 'patients',\n",
              " 'people',\n",
              " 'pgp',\n",
              " 'phone',\n",
              " 'phones',\n",
              " 'physician',\n",
              " 'pitt',\n",
              " 'pittsburgh',\n",
              " 'please',\n",
              " 'pmetzger',\n",
              " 'point',\n",
              " 'points',\n",
              " 'police',\n",
              " 'politics',\n",
              " 'postings',\n",
              " 'power',\n",
              " 'privacy',\n",
              " 'probably',\n",
              " 'program',\n",
              " 'programming',\n",
              " 'proposal',\n",
              " 'prozac',\n",
              " 'public',\n",
              " 'qualcom',\n",
              " 'ranch',\n",
              " 'range',\n",
              " 'rather',\n",
              " 're',\n",
              " 'really',\n",
              " 'reason',\n",
              " 'regarding',\n",
              " 'result',\n",
              " 'right',\n",
              " 'risk',\n",
              " 'rocket',\n",
              " 'rsa',\n",
              " 'safety',\n",
              " 'said',\n",
              " 'say',\n",
              " 'says',\n",
              " 'scheme',\n",
              " 'scientific',\n",
              " 'screen',\n",
              " 'secrecy',\n",
              " 'secret',\n",
              " 'secure',\n",
              " 'security',\n",
              " 'see',\n",
              " 'sensitivity',\n",
              " 'serial',\n",
              " 'shaft',\n",
              " 'shameful',\n",
              " 'short',\n",
              " 'should',\n",
              " 'since',\n",
              " 'single',\n",
              " 'skepticism',\n",
              " 'skin',\n",
              " 'skipjack',\n",
              " 'smb',\n",
              " 'so',\n",
              " 'software',\n",
              " 'some',\n",
              " 'someone',\n",
              " 'soon',\n",
              " 'speak',\n",
              " 'special',\n",
              " 'started',\n",
              " 'state',\n",
              " 'sternlight',\n",
              " 'still',\n",
              " 'stolen',\n",
              " 'stomach',\n",
              " 'stop',\n",
              " 'stratus',\n",
              " 'strnlght',\n",
              " 'strong',\n",
              " 'study',\n",
              " 'substances',\n",
              " 'summary',\n",
              " 'superstition',\n",
              " 'surgery',\n",
              " 'surrender',\n",
              " 'survivors',\n",
              " 'swapping',\n",
              " 'symptoms',\n",
              " 'syndrome',\n",
              " 'system',\n",
              " 'taking',\n",
              " 'talk',\n",
              " 'tapped',\n",
              " 'targets',\n",
              " 'taste',\n",
              " 'technology',\n",
              " 'telephone',\n",
              " 'tempest',\n",
              " 'test',\n",
              " 'text',\n",
              " 'thanks',\n",
              " 'that',\n",
              " 'the',\n",
              " 'their',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'think',\n",
              " 'this',\n",
              " 'time',\n",
              " 'to',\n",
              " 'toal',\n",
              " 'too',\n",
              " 'tradition',\n",
              " 'traffic',\n",
              " 'treating',\n",
              " 'treatment',\n",
              " 'trust',\n",
              " 'try',\n",
              " 'u28037',\n",
              " 'ufl',\n",
              " 'uic',\n",
              " 'uicvm',\n",
              " 'uk',\n",
              " 'university',\n",
              " 'unless',\n",
              " 'up',\n",
              " 'us',\n",
              " 'usa',\n",
              " 'usenet',\n",
              " 'using',\n",
              " 'utility',\n",
              " 'vak12ed',\n",
              " 'version',\n",
              " 'very',\n",
              " 'vga',\n",
              " 'video',\n",
              " 'viking',\n",
              " 'vms',\n",
              " 'voice',\n",
              " 'vos',\n",
              " 'wa7kgx',\n",
              " 'waco',\n",
              " 'war',\n",
              " 'warm',\n",
              " 'was',\n",
              " 'washington',\n",
              " 'we',\n",
              " 'weapon',\n",
              " 'weapons',\n",
              " 'week',\n",
              " 'well',\n",
              " 'were',\n",
              " 'what',\n",
              " 'where',\n",
              " 'whether',\n",
              " 'which',\n",
              " 'who',\n",
              " 'why',\n",
              " 'wife',\n",
              " 'will',\n",
              " 'wiretap',\n",
              " 'without',\n",
              " 'world',\n",
              " 'would',\n",
              " 'writes',\n",
              " 'year',\n",
              " 'years',\n",
              " 'yeast',\n",
              " 'you',\n",
              " 'your']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QortaJfzffYE",
        "colab_type": "text"
      },
      "source": [
        "Список какой-то странный получился: много слов не связанных с категориями (but, допустим). Числа нам не мешают: мы их не включаем в регулярку, когда пересобираем псевдотексты для новых векторов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHgLfYRrf70A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "important_words_dataRT = [pseudotext(important_wordsRT, text) for text in newsgroups_train.data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WntB809gNSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = vectorizer.transform(important_words_dataRT)\n",
        "y = newsgroups_train.target\n",
        "X, X_test, y, y_test = train_test_split(X, y, random_state=42)\n",
        "folds = KFold(n_splits=5, shuffle=True, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtifaK-CgudE",
        "colab_type": "code",
        "outputId": "273fc777-8f9e-4061-e153-ae5efeff9c8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "grid_search(ensemble.RandomForestClassifier(), parameters_RandomForest)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.7998041420079826\n",
            "Best parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 15}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39ASobUuhIAN",
        "colab_type": "code",
        "outputId": "ef47d425-39a1-4399-e2dd-36aa341560f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "cross_validation(chosenRandomForest)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-212-0bee5b7c47ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosenRandomForest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-492d6778c903>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mvalid_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cross Validation mean score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Score on test data: {0:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyiktUKmiECW",
        "colab_type": "text"
      },
      "source": [
        "Кажется, надо изменить функцию"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykfMoVsViJgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_validation(model):\n",
        "    valid_scores = []\n",
        "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
        "        X_train, X_valid = X[train_index], X[valid_index]\n",
        "        y_train, y_valid = y[train_index], y[valid_index]\n",
        "        model.fit(X_train, y_train)    \n",
        "        y_pred_test = model.predict(X_test)\n",
        "        y_pred_valid = model.predict(X_valid)\n",
        "        valid_score = sklearn.metrics.f1_score(y_valid, y_pred_valid, average='macro')\n",
        "        valid_scores.append(valid_score)\n",
        "    test_score = sklearn.metrics.f1_score(y_test, y_pred_test, average='macro')\n",
        "    print('Cross Validation mean score:', sum(valid_scores)/len(valid_scores))\n",
        "    print(\"Score on test data: {0:.4f}\".format(test_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDouo9XpieA5",
        "colab_type": "code",
        "outputId": "fd20e3f3-4e10-4ad3-9280-74a6a33fc0ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cross_validation(ensemble.RandomForestClassifier(min_samples_split=5, n_estimators=15))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Validation mean score: 0.8038521701660706\n",
            "Score on test data: 0.8151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aoeilbsjLJS",
        "colab_type": "text"
      },
      "source": [
        "Качество ухудшилось. Попробуем использовать главные слова, выбранные для модели логистической регрессии"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNbAyJKZjlBU",
        "colab_type": "code",
        "outputId": "e5ec5595-6665-45c9-fc87-086dc8ff5bb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X = vectorizer.transform(important_words_data)\n",
        "y = newsgroups_train.target\n",
        "X, X_test, y, y_test = train_test_split(X, y, random_state=42)\n",
        "folds = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "grid_search(ensemble.RandomForestClassifier(), parameters_RandomForest)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.893520757665202\n",
            "Best parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 15}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH9lZHzLkp-s",
        "colab_type": "code",
        "outputId": "6cb1a330-6966-451b-c882-b78e1ef72000",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cross_validation(ensemble.RandomForestClassifier(min_samples_split=5, n_estimators=15))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Validation mean score: 0.8927894191591517\n",
            "Score on test data: 0.8561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNdE14OJk38-",
        "colab_type": "text"
      },
      "source": [
        "Переобучение стало сильнее.\n",
        "На тестовых данных результат сопоставим с использованием всех слов, хотя во время валидации получается более высокий результат.\n",
        "Вероятно, слова с высокими весами определяют первые ветвления дерева, а фактически выбор категории зависит от слов, которые оказываются в конце цепочки.\n",
        "\n",
        "Также не исключено, что в списке весов порядок фич не соответствовал индексам в словаре.\n",
        "\n",
        "Чтобы проверить первую гипотезу, возьмём слова, вклад которых был сопоставим со средним (от 0.75 до 1.25)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXPBf6WKmGW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "important_wordsRT = []\n",
        "for i in range(len(importances)):\n",
        "    if importances[i] < 1.25*mean and importances[i] > 0.75*mean:\n",
        "        important_wordsRT.append(index_to_word[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7IMu-4rmdZ7",
        "colab_type": "code",
        "outputId": "8d3d92da-8ca7-45c6-a56b-459bb57a8c2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(important_wordsRT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['01', '10', '14', '1993apr18', '1qv83m', '2nd', '34aej7d', '408', '41', '4656', '56', '67', '7000ft', '93', '___', 'accused', 'action', 'acupuncturist', 'advocating', 'agency', 'aids', 'air', 'aisun3', 'al', 'algorithms', 'allocation', 'alt', 'american', 'another', 'arpa', 'art', 'article', 'assuming', 'at', 'australia', 'authorisation', 'authority', 'away', 'baby', 'barbecued', 'beethoven', 'before', 'bell', 'best', 'bit', 'bitmap', 'bms', 'boot', 'boxes', 'brinich', 'business', 'buy', 'by', 'capita', 'card', 'cares', 'cash', 'cbnews', 'cd', 'cellular', 'ch', 'checked', 'cheeses', 'chemically', 'chris', 'city', 'clear', 'colors', 'comments', 'communication', 'communist', 'compound', 'computing', 'confirm', 'constant', 'contact', 'continue', 'contracted', 'contributors', 'copro', 'correctly', 'council', 'creating', 'csd', 'cube', 'culture', 'cured', 'curves', 'cuts', 'cylindrical', 'dane', 'dave', 'decide', 'denver', 'depends', 'deuterium', 'devices', 'diagnosed', 'distant', 'dns1', 'donald', 'doors', 'doses', 'drawing', 'drugs', 'dry', 'dsl', 'dsto', 'duff', 'dyer', 'each', 'earl', 'earth', 'ebright', 'efforts', 'egotism', 'eis', 'enforcement', 'even', 'fda', 'federal', 'feustel', 'fifth', 'finding', 'finger', 'finish', 'firefight', 'firepower', 'folks', 'followed', 'fonts', 'forty', 'freenet', 'fuller', 'fume', 'funds', 'geoff', 'gila005', 'got', 'gov', 'goverment', 'greatly', 'guide', 'gunshot', 'handguns', 'hands', 'happening', 'happens', 'hard', 'harry', 'having', 'healy', 'heard', 'heart', 'hell', 'herringshaw', 'hismanal', 'host', 'hour', 'howard', 'hpl', 'hurt', 'husc11', 'idea', 'ideas', 'illegal', 'im4u', 'ima', 'inadmissible', 'inc', 'insane', 'insanity', 'intercon', 'interview', 'investigated', 'issue', 'janet', 'jumped', 'kadie', 'keeping', 'kind', 'knots', 'knowledge', 'knows', 'kreyling', 'lack', 'launcher', 'least', 'lerc', 'less', 'libraries', 'lightheaded', 'line', 'live', 'locations', 'locus', 'look', 'loss', 'ma', 'maintain', 'marketed', 'markets', 'maybe', 'md', 'med', 'members', 'mentioned', 'metzger', 'miller', 'million', 'months', 'most', 'multilevel', 'murder', 'must', 'n4tmi', 'ncsc', 'needs', 'neptunium', 'newsgroups', 'nf', 'nice', 'non', 'noone', 'nose', 'now', 'nsa', 'nsc', 'numbers', 'nus', 'nutrition', 'ny', 'obfuscation', 'office', 'officers', 'ones', 'operations', 'origin', 'original', 'pace', 'packard', 'paint', 'park', 'particular', 'passed', 'passphrase', 'pat', 'patents', 'paying', 'perhaps', 'peter', 'photography', 'phys', 'picture', 'plugs', 'pneumonia', 'poisoned', 'policy', 'polygons', 'port', 'position', 'posted', 'posting', 'prescription', 'president', 'procedure', 'programs', 'purchase', 'put', 'question', 'questions', 'ran', 'random', 'rate', 'ready', 'record', 'regards', 'related', 'relaxation', 'release', 'removal', 'request', 'rkba', 'robert', 'rose', 's2', 'sci', 'sean', 'semi', 'semtex', 'senator', 'send', 'severe', 'shooting', 'side', 'similar', 'small', 'snark', 'something', 'sound', 'speed', 'split', 'spooks', 'ssd', 'starting', 'statement', 'std', 'stephen', 'steve', 'stove', 'sts', 'studies', 'style', 'such', 'sudden', 'support', 'survive', 'svga', 'systems', 'tails', 'take', 'takes', 'talking', 'taos', 'tcmay', 'tcp', 'tell', 'theoretical', 'thigh', 'things', 'those', 'thousands', 'tired', 'todamhyp', 'today', 'told', 'toward', 'tps', 'treat', 'treated', 'trial', 'trials', 'two', 'tx', 'ucsu', 'udel', 'ultimate', 'understand', 'unlinfo', 'ursa', 'used', 'useful', 'utk', 'va', 'vectors', 'view', 'walls', 'wants', 'weaver', 'western', 'when', 'while', 'white', 'whoever', 'whole', 'width', 'window', 'wishes', 'within', 'work', 'write', 'writing', 'written', 'yes', 'yourself', 'zyeh']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy2RWr4vmqX4",
        "colab_type": "text"
      },
      "source": [
        "Видно, что они не соотносятся с темами.\n",
        "Наконец, посмотрим на слова, которые формируют листочки дерева"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JOvOVjIm9Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "important_wordsRT = []\n",
        "for i in range(len(importances)):\n",
        "    if importances[i] < 0.75*mean and importances[i] > 0.3*mean:\n",
        "        important_wordsRT.append(index_to_word[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk6hUlO8nKlJ",
        "colab_type": "code",
        "outputId": "2e7b6be7-c0b1-4166-c6a1-1e2c4c11200b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "important_wordsRT"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['00',\n",
              " '00101001b',\n",
              " '001230',\n",
              " '024103',\n",
              " '0952',\n",
              " '100',\n",
              " '1000',\n",
              " '126',\n",
              " '13',\n",
              " '131239',\n",
              " '13h',\n",
              " '141',\n",
              " '150',\n",
              " '15985',\n",
              " '16',\n",
              " '163133',\n",
              " '17',\n",
              " '1787',\n",
              " '1974',\n",
              " '1993',\n",
              " '1993apr14',\n",
              " '1993apr15',\n",
              " '1993apr19',\n",
              " '1993apr21',\n",
              " '1993apr5',\n",
              " '1bfdqsj53kostz6hroshsdzlvul1',\n",
              " '1qhc2p',\n",
              " '1r3jgbinn35i',\n",
              " '203',\n",
              " '2071',\n",
              " '2073',\n",
              " '21149',\n",
              " '2117',\n",
              " '214',\n",
              " '216',\n",
              " '2273',\n",
              " '2339',\n",
              " '241',\n",
              " '250',\n",
              " '26',\n",
              " '2800',\n",
              " '2ua',\n",
              " '30',\n",
              " '3060',\n",
              " '32',\n",
              " '3576',\n",
              " '37',\n",
              " '389',\n",
              " '40',\n",
              " '4615trd',\n",
              " '4660',\n",
              " '47',\n",
              " '48',\n",
              " '488',\n",
              " '505',\n",
              " '515',\n",
              " '52',\n",
              " '5363',\n",
              " '53iss6',\n",
              " '5742',\n",
              " '6000',\n",
              " '619',\n",
              " '6300',\n",
              " '640x480',\n",
              " '68',\n",
              " '704',\n",
              " '718622',\n",
              " '7800',\n",
              " '840',\n",
              " '882',\n",
              " '8k',\n",
              " '9760',\n",
              " '__the',\n",
              " '_count',\n",
              " 'aauwpiugyv2n8n',\n",
              " 'aayau',\n",
              " 'above',\n",
              " 'acaps',\n",
              " 'according',\n",
              " 'acm',\n",
              " 'activities',\n",
              " 'acts',\n",
              " 'actual',\n",
              " 'actually',\n",
              " 'address',\n",
              " 'adept',\n",
              " 'administration',\n",
              " 'administrative',\n",
              " 'adult',\n",
              " 'advance',\n",
              " 'advice',\n",
              " 'after',\n",
              " 'against',\n",
              " 'age',\n",
              " 'aggressively',\n",
              " 'aj336',\n",
              " 'ajiif6tsm',\n",
              " 'ak949',\n",
              " 'albeit',\n",
              " 'albicans',\n",
              " 'alien',\n",
              " 'almost',\n",
              " 'also',\n",
              " 'although',\n",
              " 'always',\n",
              " 'ames',\n",
              " 'amolitor',\n",
              " 'andrew',\n",
              " 'anesthetic',\n",
              " 'anger',\n",
              " 'angry',\n",
              " 'annoyed',\n",
              " 'answers',\n",
              " 'anyhow',\n",
              " 'anywhere',\n",
              " 'application',\n",
              " 'applies',\n",
              " 'appologies',\n",
              " 'appreciate',\n",
              " 'apprecied',\n",
              " 'approximate',\n",
              " 'aqueous',\n",
              " 'arc',\n",
              " 'areas',\n",
              " 'argument',\n",
              " 'arguments',\n",
              " 'argus',\n",
              " 'arkansas',\n",
              " 'arm',\n",
              " 'armond',\n",
              " 'army',\n",
              " 'aromatic',\n",
              " 'arrest',\n",
              " 'articles',\n",
              " 'artists',\n",
              " 'arythmia',\n",
              " 'asbestos',\n",
              " 'asserts',\n",
              " 'assistance',\n",
              " 'assistant',\n",
              " 'assume',\n",
              " 'assumed',\n",
              " 'atarist',\n",
              " 'athens',\n",
              " 'atrophic',\n",
              " 'att',\n",
              " 'attend',\n",
              " 'attributed',\n",
              " 'au',\n",
              " 'austin',\n",
              " 'auth',\n",
              " 'authorized',\n",
              " 'avoided',\n",
              " 'backcountry',\n",
              " 'bad',\n",
              " 'baffeled',\n",
              " 'baffled',\n",
              " 'baldwin',\n",
              " 'ballots',\n",
              " 'bandwidth',\n",
              " 'banned',\n",
              " 'bars',\n",
              " 'batch',\n",
              " 'bay',\n",
              " 'bc',\n",
              " 'bears',\n",
              " 'became',\n",
              " 'because',\n",
              " 'become',\n",
              " 'becomes',\n",
              " 'becoming',\n",
              " 'bentsen',\n",
              " 'benzodiazepine',\n",
              " 'berkom',\n",
              " 'bernstein',\n",
              " 'bert',\n",
              " 'bga',\n",
              " 'biberdorf',\n",
              " 'bigbird',\n",
              " 'bilateral',\n",
              " 'bills',\n",
              " 'bird',\n",
              " 'biting',\n",
              " 'bkw',\n",
              " 'black',\n",
              " 'blank',\n",
              " 'blood',\n",
              " 'blown',\n",
              " 'bmdelane',\n",
              " 'bmp',\n",
              " 'boise',\n",
              " 'bone',\n",
              " 'born',\n",
              " 'boston',\n",
              " 'bottle',\n",
              " 'boundary',\n",
              " 'brags',\n",
              " 'brave',\n",
              " 'breathe',\n",
              " 'brent',\n",
              " 'brings',\n",
              " 'brute',\n",
              " 'btw',\n",
              " 'bubba',\n",
              " 'build',\n",
              " 'bullet',\n",
              " 'buoyancy',\n",
              " 'butcher',\n",
              " 'butzer',\n",
              " 'byte',\n",
              " 'c5bu9m',\n",
              " 'c5l2x5',\n",
              " 'c5sv88',\n",
              " 'cadence',\n",
              " 'cadre',\n",
              " 'calculation',\n",
              " 'california',\n",
              " 'calls',\n",
              " 'camera',\n",
              " 'camp',\n",
              " 'camping',\n",
              " 'capable',\n",
              " 'capriccioso',\n",
              " 'carbon',\n",
              " 'carl',\n",
              " 'carry',\n",
              " 'cars',\n",
              " 'cases',\n",
              " 'catch',\n",
              " 'cause',\n",
              " 'causing',\n",
              " 'cb',\n",
              " 'cbmvax',\n",
              " 'celestial',\n",
              " 'center',\n",
              " 'centers',\n",
              " 'cereals',\n",
              " 'certainly',\n",
              " 'cervical',\n",
              " 'ces',\n",
              " 'cgd',\n",
              " 'chan',\n",
              " 'change',\n",
              " 'chapel',\n",
              " 'chapter',\n",
              " 'charles',\n",
              " 'chemistry',\n",
              " 'chest',\n",
              " 'chicago',\n",
              " 'children',\n",
              " 'chinet',\n",
              " 'chose',\n",
              " 'chosen',\n",
              " 'christic',\n",
              " 'christophe',\n",
              " 'cia',\n",
              " 'ciphertext',\n",
              " 'cited',\n",
              " 'civil',\n",
              " 'classified',\n",
              " 'clearly',\n",
              " 'cmort',\n",
              " 'cmp',\n",
              " 'cna',\n",
              " 'coast',\n",
              " 'cochrane',\n",
              " 'codes',\n",
              " 'collecting',\n",
              " 'colnet',\n",
              " 'colored',\n",
              " 'colour',\n",
              " 'column',\n",
              " 'comment',\n",
              " 'commerce',\n",
              " 'commercially',\n",
              " 'commit',\n",
              " 'common',\n",
              " 'commulative',\n",
              " 'communism',\n",
              " 'comp',\n",
              " 'comparison',\n",
              " 'compatible',\n",
              " 'competing',\n",
              " 'computation',\n",
              " 'computed',\n",
              " 'computerized',\n",
              " 'concern',\n",
              " 'conditional',\n",
              " 'connection',\n",
              " 'consistent',\n",
              " 'consultant',\n",
              " 'consulting',\n",
              " 'consume',\n",
              " 'contains',\n",
              " 'contractor',\n",
              " 'controlled',\n",
              " 'controls',\n",
              " 'conversion',\n",
              " 'copies',\n",
              " 'coppy',\n",
              " 'cops',\n",
              " 'copy',\n",
              " 'corporate',\n",
              " 'correction',\n",
              " 'corrections',\n",
              " 'correctness',\n",
              " 'cortical',\n",
              " 'cost',\n",
              " 'cosuard',\n",
              " 'countries',\n",
              " 'county',\n",
              " 'cover',\n",
              " 'covered',\n",
              " 'cracked',\n",
              " 'crate',\n",
              " 'create',\n",
              " 'crohn',\n",
              " 'crullerian',\n",
              " 'csie',\n",
              " 'cut',\n",
              " 'cycling',\n",
              " 'cylinder',\n",
              " 'cypherpunks',\n",
              " 'dactyl',\n",
              " 'danger',\n",
              " 'dangerous',\n",
              " 'daniel',\n",
              " 'daresay',\n",
              " 'dark',\n",
              " 'daryl',\n",
              " 'david',\n",
              " 'dc',\n",
              " 'de',\n",
              " 'deaths',\n",
              " 'decapods',\n",
              " 'dedicated',\n",
              " 'deems',\n",
              " 'deeply',\n",
              " 'defendant',\n",
              " 'definied',\n",
              " 'definitely',\n",
              " 'definition',\n",
              " 'definitive',\n",
              " 'degree',\n",
              " 'delayed',\n",
              " 'deliberate',\n",
              " 'delusional',\n",
              " 'depressingly',\n",
              " 'depth',\n",
              " 'describe',\n",
              " 'deserve',\n",
              " 'destroy',\n",
              " 'developer',\n",
              " 'development',\n",
              " 'die',\n",
              " 'different',\n",
              " 'difficult',\n",
              " 'digex',\n",
              " 'digital',\n",
              " 'digitized',\n",
              " 'dillon',\n",
              " 'dimensional',\n",
              " 'direct',\n",
              " 'direction',\n",
              " 'dirty',\n",
              " 'disclaimers',\n",
              " 'discloser',\n",
              " 'discrimination',\n",
              " 'discussed',\n",
              " 'dispense',\n",
              " 'dispersil',\n",
              " 'distinct',\n",
              " 'disturbed',\n",
              " 'documentation',\n",
              " 'doesn',\n",
              " 'doing',\n",
              " 'dol',\n",
              " 'dollar',\n",
              " 'dollars',\n",
              " 'dominance',\n",
              " 'dont',\n",
              " 'dos',\n",
              " 'doubt',\n",
              " 'douglas',\n",
              " 'dpo',\n",
              " 'dr',\n",
              " 'drag',\n",
              " 'draw',\n",
              " 'dreamer',\n",
              " 'dreams',\n",
              " 'dress',\n",
              " 'drive',\n",
              " 'drivers',\n",
              " 'dsz',\n",
              " 'dumbell',\n",
              " 'dwallach',\n",
              " 'dwestner',\n",
              " 'earlier',\n",
              " 'easily',\n",
              " 'easy',\n",
              " 'eat',\n",
              " 'economic',\n",
              " 'economy',\n",
              " 'ecpa',\n",
              " 'ed',\n",
              " 'edinburgh',\n",
              " 'education',\n",
              " 'edward',\n",
              " 'eiu',\n",
              " 'electrical',\n",
              " 'else',\n",
              " 'emphasized',\n",
              " 'enact',\n",
              " 'encrypt',\n",
              " 'enough',\n",
              " 'environment',\n",
              " 'envis2',\n",
              " 'equations',\n",
              " 'erika',\n",
              " 'errors',\n",
              " 'escalating',\n",
              " 'established',\n",
              " 'evan',\n",
              " 'exceeded',\n",
              " 'exec',\n",
              " 'existence',\n",
              " 'existent',\n",
              " 'exiting',\n",
              " 'experience',\n",
              " 'expert',\n",
              " 'explanation',\n",
              " 'exponentially',\n",
              " 'express',\n",
              " 'expressed',\n",
              " 'extra',\n",
              " 'f3',\n",
              " 'f7',\n",
              " 'facts',\n",
              " 'failing',\n",
              " 'failures',\n",
              " 'fajita19',\n",
              " 'fallen',\n",
              " 'falls',\n",
              " 'fanatical',\n",
              " 'fancy',\n",
              " 'fashionable',\n",
              " 'faster',\n",
              " 'fbihh',\n",
              " 'feel',\n",
              " 'feet',\n",
              " 'ferdinand',\n",
              " 'ferguson',\n",
              " 'fever',\n",
              " 'ffl',\n",
              " 'fg',\n",
              " 'fiberman',\n",
              " 'field',\n",
              " 'fields',\n",
              " 'fig',\n",
              " 'figures',\n",
              " 'final',\n",
              " 'finished',\n",
              " 'fires',\n",
              " 'fix',\n",
              " 'fjk6478',\n",
              " 'flavoring',\n",
              " 'fleming',\n",
              " 'flute',\n",
              " 'folk',\n",
              " 'following',\n",
              " 'foot',\n",
              " 'ford',\n",
              " 'formation',\n",
              " 'formats',\n",
              " 'foulston',\n",
              " 'frank',\n",
              " 'franklin',\n",
              " 'frenzy',\n",
              " 'front',\n",
              " 'fruit',\n",
              " 'frying',\n",
              " 'fsl',\n",
              " 'funny',\n",
              " 'future',\n",
              " 'gaffney',\n",
              " 'galactorrhea',\n",
              " 'garboc29',\n",
              " 'gates',\n",
              " 'gateway',\n",
              " 'gave',\n",
              " 'gemini',\n",
              " 'general',\n",
              " 'german',\n",
              " 'get',\n",
              " 'gks',\n",
              " 'gladly',\n",
              " 'gmark',\n",
              " 'gms',\n",
              " 'gmt',\n",
              " 'go',\n",
              " 'goa',\n",
              " 'gordon_sumerling',\n",
              " 'gorelick',\n",
              " 'gp',\n",
              " 'grady',\n",
              " 'granted',\n",
              " 'grasses',\n",
              " 'great',\n",
              " 'greatest',\n",
              " 'groups',\n",
              " 'gt6511a',\n",
              " 'guarantee',\n",
              " 'guest',\n",
              " 'gum',\n",
              " 'gyn',\n",
              " 'gynecological',\n",
              " 'haaden',\n",
              " 'hahn',\n",
              " 'handjobs',\n",
              " 'handle',\n",
              " 'harris',\n",
              " 'harvey',\n",
              " 'hasn',\n",
              " 'hate',\n",
              " 'hawaii',\n",
              " 'headache',\n",
              " 'headaches',\n",
              " 'header',\n",
              " 'hear',\n",
              " 'held',\n",
              " 'helix',\n",
              " 'helpful',\n",
              " 'hen',\n",
              " 'herndon',\n",
              " 'history',\n",
              " 'hkuxb',\n",
              " 'hmm',\n",
              " 'hmmm',\n",
              " 'hooch',\n",
              " 'hopkins',\n",
              " 'horowitz',\n",
              " 'houston',\n",
              " 'however',\n",
              " 'huxley',\n",
              " 'ian',\n",
              " 'ic',\n",
              " 'idbsu',\n",
              " 'identify',\n",
              " 'idr',\n",
              " 'ifas',\n",
              " 'iff',\n",
              " 'ignite',\n",
              " 'ignorant',\n",
              " 'ignore',\n",
              " 'ill',\n",
              " 'illeagle',\n",
              " 'immaturity',\n",
              " 'immediately',\n",
              " 'immune',\n",
              " 'immunoglobin',\n",
              " 'impaired',\n",
              " 'include',\n",
              " 'included',\n",
              " 'including',\n",
              " 'incompetence',\n",
              " 'incurable',\n",
              " 'indiana',\n",
              " 'individual',\n",
              " 'individuals',\n",
              " 'induce',\n",
              " 'industry',\n",
              " 'inet',\n",
              " 'infinite',\n",
              " 'info',\n",
              " 'informed',\n",
              " 'ins',\n",
              " 'insert',\n",
              " 'inside',\n",
              " 'inspired',\n",
              " 'installations',\n",
              " 'instead',\n",
              " 'institute',\n",
              " 'instructions',\n",
              " 'integer',\n",
              " 'intel',\n",
              " 'intended',\n",
              " 'intentional',\n",
              " 'interested',\n",
              " 'interesting',\n",
              " 'intergraph',\n",
              " 'interpretations',\n",
              " 'introduction',\n",
              " 'inundate',\n",
              " 'inventors',\n",
              " 'involving',\n",
              " 'iowa',\n",
              " 'ip',\n",
              " 'issa',\n",
              " 'itd',\n",
              " 'jagani',\n",
              " 'jason',\n",
              " 'jbotz',\n",
              " 'jchen',\n",
              " 'jebright',\n",
              " 'jeeves',\n",
              " 'jerry',\n",
              " 'jfreund',\n",
              " 'jgk',\n",
              " 'jh224',\n",
              " 'joachim',\n",
              " 'job',\n",
              " 'john',\n",
              " 'joke',\n",
              " 'jonathan',\n",
              " 'kagalenko',\n",
              " 'keane',\n",
              " 'keith',\n",
              " 'kenneth',\n",
              " 'kept',\n",
              " 'kevin',\n",
              " 'keyphrase',\n",
              " 'kgnvmy',\n",
              " 'kilty',\n",
              " 'kinds',\n",
              " 'kisses',\n",
              " 'knew',\n",
              " 'koelln',\n",
              " 'kxgst1',\n",
              " 'kyanko',\n",
              " 'la',\n",
              " 'lab',\n",
              " 'laboratories',\n",
              " 'labs',\n",
              " 'lamp',\n",
              " 'lance',\n",
              " 'larger',\n",
              " 'lately',\n",
              " 'launch',\n",
              " 'layout',\n",
              " 'lb',\n",
              " 'lcs',\n",
              " 'leading',\n",
              " 'led',\n",
              " 'lens',\n",
              " 'leonard',\n",
              " 'libertarian',\n",
              " 'library',\n",
              " 'license',\n",
              " 'life',\n",
              " 'light',\n",
              " 'liked',\n",
              " 'limit',\n",
              " 'limits',\n",
              " 'lindsay',\n",
              " 'linkoping',\n",
              " 'lips',\n",
              " 'list',\n",
              " 'listened',\n",
              " 'lists',\n",
              " 'listserv',\n",
              " 'literally',\n",
              " 'little',\n",
              " 'll',\n",
              " 'login',\n",
              " 'lonestar',\n",
              " 'longer',\n",
              " 'looks',\n",
              " 'los',\n",
              " 'lot',\n",
              " 'lots',\n",
              " 'low',\n",
              " 'lt8keoinn31v',\n",
              " 'lydick',\n",
              " 'm2c',\n",
              " 'mac',\n",
              " 'madvax',\n",
              " 'magazine',\n",
              " 'magnitude',\n",
              " 'mailsafe',\n",
              " 'major',\n",
              " 'male',\n",
              " 'malo',\n",
              " 'manual',\n",
              " 'manufacture',\n",
              " 'many',\n",
              " 'marginally',\n",
              " 'marie',\n",
              " 'marietta',\n",
              " 'mark',\n",
              " 'marlena',\n",
              " 'masada',\n",
              " 'massachusetts',\n",
              " 'massacre',\n",
              " 'material',\n",
              " 'math',\n",
              " 'mathematics',\n",
              " 'mathew',\n",
              " 'matusevich',\n",
              " 'maumee',\n",
              " 'mcgill',\n",
              " 'md5ofpublickey',\n",
              " 'means',\n",
              " 'measure',\n",
              " 'measured',\n",
              " 'measures',\n",
              " 'mechanisms',\n",
              " 'medical',\n",
              " 'member',\n",
              " 'memorable',\n",
              " 'merely',\n",
              " 'merill',\n",
              " 'messy',\n",
              " 'met',\n",
              " 'method',\n",
              " 'methods',\n",
              " 'mexico',\n",
              " 'mi6',\n",
              " 'mia',\n",
              " 'mild',\n",
              " 'military',\n",
              " 'militia',\n",
              " 'milk',\n",
              " 'minded',\n",
              " 'minute',\n",
              " 'misinformation',\n",
              " 'misinterretation',\n",
              " 'mitchell',\n",
              " 'mizzou1',\n",
              " 'modify',\n",
              " 'moment',\n",
              " 'mon',\n",
              " 'money',\n",
              " 'monitor',\n",
              " 'monitoring',\n",
              " 'more',\n",
              " 'mormons',\n",
              " 'motif',\n",
              " 'mourning',\n",
              " 'move',\n",
              " 'movement',\n",
              " 'mpkjao',\n",
              " 'mpm',\n",
              " 'ms',\n",
              " 'msg',\n",
              " 'msk',\n",
              " 'mst',\n",
              " 'mudd',\n",
              " 'muenchen',\n",
              " 'musings',\n",
              " 'myopia',\n",
              " 'name',\n",
              " 'namely',\n",
              " 'nasa',\n",
              " 'national',\n",
              " 'navy',\n",
              " 'nazi',\n",
              " 'ncsl',\n",
              " 'necessity',\n",
              " 'needle',\n",
              " 'negotiations',\n",
              " 'nerve',\n",
              " 'networking',\n",
              " 'neustaedter',\n",
              " 'newsreader',\n",
              " 'newton',\n",
              " 'next',\n",
              " 'nicholas',\n",
              " 'nico',\n",
              " 'night',\n",
              " 'nine',\n",
              " 'nishi',\n",
              " 'nlp',\n",
              " 'nmsu',\n",
              " 'nmt',\n",
              " 'nntp',\n",
              " 'normally',\n",
              " 'northwest',\n",
              " 'nosc',\n",
              " 'note',\n",
              " 'noticed',\n",
              " 'notion',\n",
              " 'novelist',\n",
              " 'nsf',\n",
              " 'number',\n",
              " 'numerical',\n",
              " 'nutcase',\n",
              " 'nyeda',\n",
              " 'nym',\n",
              " 'nz',\n",
              " 'oath',\n",
              " 'obelix',\n",
              " 'objective',\n",
              " 'obviously',\n",
              " 'occur',\n",
              " 'odd',\n",
              " 'off',\n",
              " 'officials',\n",
              " 'oh',\n",
              " 'ohio',\n",
              " 'ok',\n",
              " 'old',\n",
              " 'omitted',\n",
              " 'onto',\n",
              " 'oook',\n",
              " 'organization',\n",
              " 'originator',\n",
              " 'orleans',\n",
              " 'orthpod',\n",
              " 'os',\n",
              " 'osler',\n",
              " 'ossd',\n",
              " 'ottawa',\n",
              " 'outline',\n",
              " 'outweigh',\n",
              " 'overreact',\n",
              " 'own',\n",
              " 'owner',\n",
              " 'ownership',\n",
              " 'p003228',\n",
              " 'pa',\n",
              " 'pacifist',\n",
              " 'panics',\n",
              " 'partnership',\n",
              " 'passe',\n",
              " 'passes',\n",
              " 'passionate',\n",
              " 'paul',\n",
              " 'pcd7',\n",
              " 'pd',\n",
              " 'performs',\n",
              " 'permission',\n",
              " 'persecute',\n",
              " 'person',\n",
              " 'pete',\n",
              " 'philippines',\n",
              " 'photo',\n",
              " 'photon',\n",
              " 'pilot',\n",
              " 'pistols',\n",
              " 'pixel',\n",
              " 'place',\n",
              " 'plains',\n",
              " 'plane',\n",
              " 'plate',\n",
              " 'plunge',\n",
              " 'pm',\n",
              " 'pobox',\n",
              " 'pope',\n",
              " 'population',\n",
              " 'portal',\n",
              " 'positions',\n",
              " 'possible',\n",
              " 'possibly',\n",
              " 'pound',\n",
              " 'practical',\n",
              " 'practitioner',\n",
              " 'predecesso',\n",
              " 'preferably',\n",
              " 'pregnancy',\n",
              " 'presence',\n",
              " 'preserving',\n",
              " 'press',\n",
              " 'pressure',\n",
              " 'prevent',\n",
              " 'pricing',\n",
              " 'primary',\n",
              " 'prince',\n",
              " 'principles',\n",
              " 'printing',\n",
              " 'prng',\n",
              " 'problem',\n",
              " 'procedures',\n",
              " 'processor',\n",
              " 'product',\n",
              " 'products',\n",
              " 'profile',\n",
              " 'project',\n",
              " 'proof',\n",
              " 'properly',\n",
              " 'property',\n",
              " 'proposing',\n",
              " 'prosecute',\n",
              " 'protect',\n",
              " 'protected',\n",
              " 'protocols',\n",
              " 'ps',\n",
              " 'pst',\n",
              " 'psu',\n",
              " 'pt',\n",
              " 'published',\n",
              " 'publishing',\n",
              " 'pun',\n",
              " 'purchased',\n",
              " 'putting',\n",
              " 'quadrilateral',\n",
              " 'quantity',\n",
              " 'quite',\n",
              " 'quotable',\n",
              " 'quotations',\n",
              " 'quote',\n",
              " 'quoting',\n",
              " 'rachmat',\n",
              " 'radiation',\n",
              " 'radical',\n",
              " 'radio',\n",
              " 'ram',\n",
              " 'randomly',\n",
              " 'rapes',\n",
              " 'raw',\n",
              " 'rd',\n",
              " 'reach',\n",
              " 'reagardless',\n",
              " 'reality',\n",
              " 'reasonable',\n",
              " 'reasoned',\n",
              " 'reasoning',\n",
              " 'recently',\n",
              " 'reconstruct',\n",
              " 'recovery',\n",
              " 'reference',\n",
              " 'referred',\n",
              " 'refuses',\n",
              " 'regardless',\n",
              " 'register',\n",
              " 'relatively',\n",
              " 'remember',\n",
              " 'rene',\n",
              " 'reno',\n",
              " 'rep',\n",
              " 'repeat',\n",
              " 'replied',\n",
              " 'replies',\n",
              " 'reply',\n",
              " 'reports',\n",
              " 'representative',\n",
              " 'represented',\n",
              " 'requesting',\n",
              " 'require',\n",
              " 'required',\n",
              " 'responded',\n",
              " 'responsible',\n",
              " 'restaurant',\n",
              " 'restaurants',\n",
              " 'rgc3679',\n",
              " 'rice',\n",
              " 'richard',\n",
              " 'riddle',\n",
              " 'riordan',\n",
              " 'riot',\n",
              " 'road',\n",
              " 'robbery',\n",
              " 'rock',\n",
              " 'rogers',\n",
              " 'rohypnol',\n",
              " 'ron',\n",
              " 'rouvalis',\n",
              " 'rpi',\n",
              " 'rss',\n",
              " 'rubber',\n",
              " 'rubicks',\n",
              " 'rumours',\n",
              " 'run',\n",
              " 'runs',\n",
              " 'rusage',\n",
              " 'russians',\n",
              " 'sail',\n",
              " 'sale',\n",
              " 'salesman',\n",
              " 'same',\n",
              " 'san',\n",
              " 'saved',\n",
              " 'saw',\n",
              " 'scale',\n",
              " 'scaled',\n",
              " 'scape',\n",
              " 'scarolina',\n",
              " 'sccsi',\n",
              " 'scheduled',\n",
              " 'scientists',\n",
              " 'scope',\n",
              " 'scramble',\n",
              " 'scws7',\n",
              " 'second',\n",
              " 'seem',\n",
              " 'seems',\n",
              " 'seizures',\n",
              " 'sell',\n",
              " 'senate',\n",
              " 'sense',\n",
              " 'seperable',\n",
              " 'seperate',\n",
              " 'series',\n",
              " 'server',\n",
              " 'service',\n",
              " 'services',\n",
              " 'set',\n",
              " 'setting',\n",
              " 'shankley',\n",
              " 'shapiro',\n",
              " 'sheet',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKgWfciBnjLN",
        "colab_type": "text"
      },
      "source": [
        "По-прежнему выглядит так себе, но попробуем использовать их"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhSLHSYqnzOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "important_words_dataRT = [pseudotext(important_wordsRT, text) for text in newsgroups_train.data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "064kSDxLoaV1",
        "colab_type": "code",
        "outputId": "41492bfd-b7e1-43ef-9273-536f0bee261d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X = vectorizer.transform(important_words_dataRT)\n",
        "y = newsgroups_train.target\n",
        "X, X_test, y, y_test = train_test_split(X, y, random_state=42)\n",
        "folds = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "cross_validation(chosenRandomForest)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Validation mean score: 0.6821735136052991\n",
            "Score on test data: 0.6836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h24GSLRovWm",
        "colab_type": "text"
      },
      "source": [
        "На листочках получается совсем плохо"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAlTRu7D-SKi",
        "colab_type": "text"
      },
      "source": [
        "K ближайших соседей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azPtWvlj-c9P",
        "colab_type": "text"
      },
      "source": [
        "Воспользуемся словами, у которых были наибольшие веса в логистической регрессии"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_4YqYLo-Y37",
        "colab_type": "code",
        "outputId": "738e99a5-8ebb-4868-cfcc-ebe22b6a71ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X = vectorizer.transform(important_words_data)\n",
        "y = newsgroups_train.target\n",
        "X, X_test, y, y_test = train_test_split(X, y, random_state=42)\n",
        "folds = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "from sklearn import neighbors\n",
        "grid_search(neighbors.KNeighborsClassifier(), parameters_kNeigbors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.7096129798383806\n",
            "Best parameters: {'n_neighbors': 3, 'p': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En-ARq3DBxf-",
        "colab_type": "code",
        "outputId": "f5e9c286-d969-463d-a520-afb029ee92c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cross_validation(neighbors.KNeighborsClassifier(n_neighbors=3, p=2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Validation mean score: 0.7096129798383807\n",
            "Score on test data: 0.7164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTgXKC70CKal",
        "colab_type": "text"
      },
      "source": [
        "Качество немного повысилась, переобучения нет.\n",
        "Методы извлечения информации о словах с самым большим весом, которые работали для логистической  регрессии и для случайного леса, для K ближайших соседей не подходят."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn0DDwKcEdqm",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxhqvaQ0EfzN",
        "colab_type": "text"
      },
      "source": [
        "Подберём параметры CountVectirizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77wHOa5AEowd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chooseCVparams(model):\n",
        "    ngram_range = [(1, 1), (1, 2)]\n",
        "    max_df_list = [None, 1, 2]\n",
        "    min_df_list = [None, 0.5, 1]\n",
        "    max_features_list = [None, 500]\n",
        "    for ngram_range in ngrams:\n",
        "        for max_df in max_df_list:\n",
        "            for min_df in min_df_list:\n",
        "                  for max_features in max_features_list:\n",
        "                      try: #на случай, если есть несовместимые комбинации параметров\n",
        "                            vectorizer = sklearn.feature_extraction.text.CountVectorizer(max_df=max_df, min_df=min_df,max_features=max_features)\n",
        "                            X=vectorizer.fit_transform(newsgroups_train.data)\n",
        "                            print(vectorizer)\n",
        "                            cross_validation(model)\n",
        "                        except:\n",
        "                             pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EVU4hlHDtIX",
        "colab_type": "code",
        "outputId": "50a3f686-f6af-4472-ee91-60aa9e5f7e68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "chooseCVparams(chosenLogisticRegression)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.938068071731182\n",
            "Score on test data: 0.9501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UdakrE0TahR",
        "colab_type": "text"
      },
      "source": [
        "Результат везде одинаковый, что странно. На других моделях (см. ниже) результат зависит от параметров векторайзера. Возможно, логистическая регрессия сама выбирает важные слова так хорошо, что настройки векторайзера не принципиально. Правда, остаётся загадкой, почему не влияют настройки биграм."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYBvVTZsQeor",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c19d6faf-8325-4b44-b0d8-eca203ce4d16"
      },
      "source": [
        "chooseCVparams(chosenRandomForest)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.8652434500838151\n",
            "Score on test data: 0.8607\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.8627445637304301\n",
            "Score on test data: 0.8843\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.8559499221337044\n",
            "Score on test data: 0.8392\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.8581070375339314\n",
            "Score on test data: 0.8297\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.862537268048459\n",
            "Score on test data: 0.8383\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.8528329664832758\n",
            "Score on test data: 0.8633\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.8676097909320863\n",
            "Score on test data: 0.8355\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.8687336351266257\n",
            "Score on test data: 0.8487\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.8720385308007546\n",
            "Score on test data: 0.8682\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.8726625033655612\n",
            "Score on test data: 0.8572\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.8523466294654827\n",
            "Score on test data: 0.8524\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.8536516505290225\n",
            "Score on test data: 0.8705\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.8549999597333159\n",
            "Score on test data: 0.8636\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.8675233863484997\n",
            "Score on test data: 0.8374\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.8625051620616005\n",
            "Score on test data: 0.8384\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.874354923873734\n",
            "Score on test data: 0.8651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh7jGy_SUakw",
        "colab_type": "text"
      },
      "source": [
        "Лучший результат случайного леса на тесте:\n",
        "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
        "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
        "                lowercase=True, max_df=2, max_features=1000, min_df=1,\n",
        "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
        "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "                tokenizer=None, vocabulary=None)\n",
        "Cross Validation mean score: 0.8536516505290225\n",
        "Score on test data: 0.8705"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrUFTszdVlyG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b1f3df3b-4083-4319-ebfe-48439a65753b"
      },
      "source": [
        "chooseCVparams(chosenKNeigbors)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n",
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=2, max_features=1000, min_df=1,\n",
            "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "Cross Validation mean score: 0.694327666230924\n",
            "Score on test data: 0.7040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz7nuTzXV6Dc",
        "colab_type": "text"
      },
      "source": [
        "Для метода K ближайших соседей настройки векторайзера тоже оказались не принципиальны."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBPMHQBZV6Kn",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-gz28eJV6O5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}